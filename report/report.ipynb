{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Theory Questions\n",
    "\n",
    "#### MLE\n",
    "\n",
    "<img src=\"Q1.png\">\n",
    "<img src=\"A1.png\">\n",
    "<img src=\"A2.png\">\n",
    "<img src=\"a3.png\">\n",
    "<br>\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "<img src=\"naive_q1.png\">\n",
    "<img src=\"naive_1.png\">\n",
    "<img src=\"naive_q2.png\">\n",
    "<img src=\"naive_2.png\">\n",
    "\n",
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                     PART II: Sentiment Analysis with Naive Bayes\n",
    "\n",
    "#### About Naive Bayes\n",
    "\n",
    "- In this assignment I will implement a Multinomial Naive Bayes classifier and verify its performance on the given sentiment dataset. Before starting report explanation let me briefly explain Naive Bayes.\n",
    "- Naive Bayes methods are set of supervised learning algorithms based on conditional independence between every pair of feature given, after that predict the outcome of later probabilities. In the Bayesian methods priori probability is important. Bayesian theorem have many application in NLP especially classification problems.\n",
    "   <img src=\"Bayes_basic.png\">\n",
    "- There are two main advantages in Naive Bayes algorithms:\n",
    "    - Can learn the paramters with small amount of data.\n",
    "    - Train fast.\n",
    "- But there are two main disadvantages:\n",
    "    - It works well with discrete values but not work with continuous values.\n",
    "    - Even if it is a good classifier, it is a bad estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier for Text Analysis\n",
    "\n",
    "- MNB (MultinomialNB) is one of the two main NB implementation used in text classification. Data typically reporesented as word vector counts. Ɵy= (Ɵy1,Ɵy2,...,Ɵyn); n is the number of features, y is the class label and Ɵyi is the probability of the feature.\n",
    "- The parameters is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "- However some words may have 0 counts. In order to avoid these bad conditions we use Laplace Smoothing with value of α=1. Nyi is the number of times feature  appears in a sample of class  in the training set. Ny is the total count of all features for class y.\n",
    "<img src=\"oran.png\">\n",
    "- The Multinomial Naive Bayes classifier becomes a linear classifier when expressed in log-space:\n",
    "<img src=\"mnb_formula.png\"> Taken from wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Understanding the data\n",
    "\n",
    "- According to the accuracy results of the MNB with BoW, classification and the six_category_classification is very feasible. Before  the implementation I assume that adjectives (such as good, not good or bad) would be the main keywords for review estimations.\n",
    "\n",
    "#### 2. Implementing Naive Bayes\n",
    "\n",
    "- Main function is mnb_score_calculator and it takes two parameter, first one is ngram range and the second one is what we want to classify(pos-neg or six-categories). Count vectorizer is chosen and it uses its own dictionary. After train and test sets were handled, we fit the train set with our implemented MNB class. Before explain the coding part. Discuss about BoW:\n",
    "    - BOW model is used for feature extraction in text data. \n",
    "    - It holds the words and the number of times their occurence.\n",
    "    - It is good to ignore the stop words,uppercase letters to reduce the length of the BoW model.\n",
    "- My implemented MNB code basicly two main function. Fit and predict.\n",
    "    - Fit firstly creates an class variable and it has length of 2. (positive and negative)\n",
    "    - Prior was calculated seperately for each class(positive and negative), length of positives(or neg)/ length of total number of train set.\n",
    "    - Likelihood is calculated seperately for each class, (sum of positive (or neg) element in the positive set + alfa value(Laplace smoothing)) (dividing by sum of positive set +alfa value)\n",
    "    <hr>\n",
    "    - After train was done: Next is predict function:\n",
    "    - Main idea is to calculate the posteriori and it is done by priori and the likelihood\n",
    "<img src=\"post=pri.png\">\n",
    "    - Taking log of the likelihood and priori is to avoid from the underflow. This calculation is linear classifier. Because making classification decision based off a linear combination of probabilities.\n",
    "    - Finally accuracy is calculated. By the given formula\n",
    "\n",
    "<hr>\n",
    "\n",
    "- As you can see the code part, there are 6 types of accuracy calculation. First there accuracy(unigram-bigram and both) are for positive-negative and last three are for the six category classifier. \n",
    "\n",
    " - ###### About N-gram : \n",
    " \n",
    "     - Human can understand the language and sentences but machines can not. So that some patterns are developped to make the machines understand. N-grams are basically set of occuring words within given n-size. And for a sentence\n",
    "      Today is the worst day of my life\n",
    "      - For n=1 it is Unigram: Only consider word one by one which is unigram so each word will be one gram. \"Today\" \"is\" \"the\" \"worst\" \"day\" \"of\" \"my\" \"life\"\n",
    "      - For n=2 it is bigram: Consider two adjacent words at a time. \"Today is\", \"is the\", \"the worst\", \"worst day\", \"day of\", \"of my\", \"my life\". \n",
    "\n",
    "<hr>      \n",
    "    \n",
    "\n",
    "#### 3. Error Analysis\n",
    "\n",
    "- In statistical classification, the Bayes classifier minimizes the probability of misclassification. Alpha value can also change the classification result. But...\n",
    "- Naive Bayes classifier makes such assumptions when they are not generally correct in real-world situations. Naive Bayes classifiers are not sensitive to irrelevant features.\n",
    "- Due to such strong and simple assumptions, this family of classifiers is called Naive.  If there are any existing dependencies, this classifier cannot modify them in any way other than to eliminate dependencies simply by ignoring them.\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### 4. Modul Analysis\n",
    "\n",
    " Note:In this part only the unigram part is considered. Because others do the same operations.\n",
    " \n",
    "###### a)Analyzing effect of the words on prediction\n",
    "\n",
    "- There are underused words with a high proportion because they are used only in one classroom. These can not be trusted. In the figure below (debug was used to show) we can see the list of the words whose presence most strongly predicts that the review is positive.  \n",
    "- Nonsense but if one them used in a word then the sentence most strongly predicted like that. \n",
    "<img src=\"pos_rate.png\">\n",
    "<hr>\n",
    "<br>\n",
    "- There is a list of words whose absence most strongly predicts that the review is positive. These are the common words and trusted. These words take their power by their from their use. If these are not used in a sentence the review most probably is positive. Also as you can see these are the adjectives we use to describe bad. \n",
    "<img src=\"neg_common.png\">\n",
    "<hr>\n",
    "<br>\n",
    "- The same condition also occurs on the list of the words whose presence most strongly predicts that the review is negative. Again these are not trusted and nonsense.\n",
    "<img src=\"neg_rate.png\">\n",
    "<hr>\n",
    "<br>\n",
    "- There is a list of words whose absence most strongly predicts that the review is negative. As you can see they are adjectives we use to describe good. And they are trusted.\n",
    "<img src=\"pos_common.png\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "In here I obtain them without discarding stopwords. Even though ignoring them is good with this large dataset. But it is used in the below part to explain.\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "###### b) Stopwords\n",
    "\n",
    "- I imported the from sklearn.feature extraction.text import ENGLISH STOP WORDS. But I did not use to show in here.\n",
    "<img src=\"stopwords.png\">\n",
    "- As you can see in the above using stopwords in this large dataset is not needed. They are used almost every sentence. Because their presense or absense changes nothing.\n",
    "\n",
    "\n",
    "###### c) Analyzing effect of the stopwords\n",
    "\n",
    " - From an example:\n",
    "    - Atakan is not good.\n",
    "\n",
    "\n",
    "- If we ignore the stopwords the result would be [\"Atakan\",\"good\"]. In here result is positive but in reality it is not. This is one problem and another problem of removing stop words from the model is that it’s crucial to have these tokens when our goal is to generate text or to work with search engines.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "- So when should I remove stopwords?\n",
    "    - Remove these tokens only if they don’t add any new information for your problem. Classification problems normally don’t need stop words because it’s possible to talk about the general idea of a text even if you remove stop words from it. And removing them increases the performance.\n",
    "<hr>\n",
    "Anyway, you should never remove stop words without thinking about the impact of these words on the problem you are trying to solve.\n",
    "\n",
    "#### 5. Calculation of Accuracy\n",
    "\n",
    "- There are three output for positive-negative classification. And these three accuracies are for unigram, bigram and both cases.\n",
    "\n",
    "- The same accuracy is calculated for the six-category classification. As you can see using stopwords gets better accuracy but it decreases the performance.\n",
    "\n",
    "<img src=\"accuracies.png\">\n",
    "\n",
    "#### 6. (Bonus) Implement a six-category classifier\n",
    "\n",
    "This almost changes nothing. I just change the Y_label of the data. And the result can be seen figure above.\n",
    "<img src=\"load_data.png\">\n",
    "When I switch the six_category boolean value to True the code implement itself as  a six-category classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
